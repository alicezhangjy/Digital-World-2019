# -*- coding: utf-8 -*-
"""
Created on Wed Apr 10 21:40:53 2019

@author: Alice Jingyu
"""

# Question 1
import numpy as np
from sklearn.metrics import confusion_matrix

def get_metrics(actual_targets, predicted_targets, labels):
    c_matrix = confusion_matrix(actual_targets, predicted_targets, labels)
    total_records = len(actual_targets)
    accuracy = round((c_matrix[0,0]+c_matrix[1,1])/total_records , 3)
    sensitivity = round((c_matrix[1,1])/(c_matrix[1,0]+c_matrix[1,1]) , 3)
    false_positive = round((c_matrix[0,1])/(c_matrix[0,0]+c_matrix[0,1]) , 3)
    
    output = {'confusion matrix':c_matrix,'total records':total_records,'accuracy':accuracy,'sensitivity':sensitivity,'false positive rate':false_positive}
    return output

#Question 2
import numpy as np

def five_number_summary(x):
    if len(x.shape) == 1:
        return None 
    
    list_ = []
    for i in range(x.shape[1]):
        col = x[:,i]
        dictionary = {'minimum': np.min(col),'first quartile': np.percentile(col,25),'median': np.median(col),'third quartile': np.percentile(col,75),'maximum': np.max(col)}
        list_.append(dictionary)
    return list_

#Question 3
import numpy as np

def normalize_minmax(data):

    size = data.shape
    if (len(size) == 1):
        return None
    else:
        columns = size[1]
    for i in range(columns):
        maximum = np.max(data[:,i])
        minimum = np.min(data[:,i])
        denominator = maximum - minimum
        
        data[:,i] = (data[:,i] - minimum) / denominator
    return data
        
#Question 4
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split 
from sklearn import neighbors, datasets
from sklearn.metrics import confusion_matrix
import numpy as np
bunchobject = datasets.load_breast_cancer()

def normalize_minmax(x):
    if len(x.shape) != 2:
        return None
    else:
        
    
        for i in range(x.shape[1]):
            x[:,i] = (x[:,i] - np.min(x[:,i]))/(np.max(x[:,i])-np.min(x[:,i]))

        return x
    
def get_metrics(actual_targets, predicted_targets, labels):
    c_matrix = confusion_matrix(actual_targets, predicted_targets,labels)
    
    d = {'confusion matrix':np.round(c_matrix,3),
         'total records':np.round(np.sum(c_matrix),3),
         'accuracy':np.round((c_matrix[0][0]+c_matrix[1][1])/np.sum(c_matrix),3),
         'sensitivity':np.round(c_matrix[1][1]/np.sum(c_matrix[1]),3),
         'false positive rate':np.round(c_matrix[0][1]/np.sum(c_matrix[0]),3)
    }
    
    return d

    
def knn_classifier(bunchobject, feature_list, size, seed, k):

    data = bunchobject.data[:, feature_list]
    target = bunchobject.target

    data = normalize_minmax(data)

    data_train, data_test, target_train, target_test = train_test_split(data , target , test_size = size, random_state = seed )

    clf = neighbors.KNeighborsClassifier(k)
    clf.fit(data_train,target_train)
    target_predicted = clf.predict(data_test)
    results = get_metrics(target_test,target_predicted,[1,0]) 
    return results

#Question 5
    import numpy as np
from sklearn import linear_model 
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn import datasets
import matplotlib.pyplot as plt
bunchobject = datasets.load_breast_cancer()



def display_scatter(x,y,xlabel = 'x', ylabel ='y', title_name ='default'):
    x_index = 0
    y_index = 3
    x = bunchobject.data[:,[x_index]]
    y = bunchobject.data[:,[y_index]]
    x_label = bunchobject.feature_names[x_index]
    y_label = bunchobject.feature_names[y_index]
    fig,ax  = plt.subplots()
    ax.scatter(x,y)
    plt.show()


def linear_regression(bunchobject, x_index, y_index, size, seed):
    x_train, x_test, y_train, y_test = train_test_split(bunchobject.data[:,[x_index]], \
                                                        bunchobject.data[:,[y_index]], \
                                                        test_size = size, \
                                                        random_state = seed)
    
    regr = linear_model.LinearRegression()
    regr.fit(x_train,y_train)
    print(regr.coef_, regr.intercept_)
    y_pred = regr.predict(x_test)
    MSE = mean_squared_error(y_test,y_pred)
    R2 = r2_score(y_test,y_pred)
    results = {'coefficients':np.array(regr.coef_),'intercept': np.array(regr.intercept_),'mean squared error': MSE,'r2 score': R2}
    print("MSE:", MSE , "R2:", R2)
    return x_train,y_train, x_test, y_pred, results

def plot_linear_regression(x1,y1,x2,y2,x_label='',y_label=''):
    plt.scatter(x1,y1,color='black')
    pass

#Question 6
import numpy as np
from sklearn import linear_model 
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn import datasets
import matplotlib.pyplot as plt
bunchobject = datasets.load_breast_cancer()

def multiple_linear_regression(bunchobject, x_index, y_index, order, size, seed):
    poly = PolynomialFeatures(order, include_bias = False)
    c = poly.fit_transform(bunchobject.data[:,[x_index]])
    c_train, c_test , y_train , y_test = train_test_split(c,\
                                                          bunchobject.data[:,[y_index]],\
                                                          test_size = size,\
                                                          random_state = seed)
    regr = linear_model.LinearRegression()
    regr.fit(c_train,y_train)
    y_pred = regr.predict(c_test)
    MSE = mean_squared_error(y_test,y_pred)
    R2 = r2_score(y_test,y_pred)
    results = {'coefficients':np.array(regr.coef_),'intercept': np.array(regr.intercept_),'mean squared error': MSE,'r2 score': R2}
    return c_train[:,[0]], y_train, c_test[:,[0]], y_pred, results  #make sure to change ctrain to first column only cause only want x values.

def plot_linear_regression(x1,y1,x2,y2,x_label='',y_label=''):
    plt.scatter(x1,y1,color='black')
    plt.scatter(x2,y2,color= 'blue')
    plt.show()


x_train, y_train , x_test, y_pred, results = multiple_linear_regression(bunchobject,0,3,4,0.4,2752)
print(results)
#plot_linear_regression(x_train, y_train, x_test, y_pred,bunchobject.feature_names[0],bunchobject.feature_names[3])

#Question 7
import numpy as np
from sklearn import neighbors, datasets
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split

def get_metrics(actual_targets, predicted_targets, labels):
    c_matrix = confusion_matrix(actual_targets, predicted_targets, labels)
    total_records = c_matrix.sum()
    accuracy = (c_matrix[0,0] + c_matrix[1,1]) / total_records 
    sensitivity = c_matrix[1,1] / (c_matrix[1,1] + c_matrix[1,0])
    false_positive_rate = c_matrix[0,1] / np.sum(c_matrix[0,:])
    
    output = {
       'confusion matrix': c_matrix, 
       'total records': int(total_records),
       'accuracy': round(accuracy,3), 
        'sensitivity': round(sensitivity,3),
       'false positive rate': round(false_positive_rate,3)
    }
    
    return output

def normalize_minmax(data):
    size = data.shape
    
    if len(size) == 1:
        return
    else:
        columns = size[1]
        
    for i in range(columns):
        maximum = np.max(data[:,i])
        minimum = np.min(data[:,i])
        denominator = maximum - minimum

        data[:,i]=(data[:,i]- minimum)/denominator
    return data

def knn_classifier_full(bunchobject, feature_list, size, seed):       
    data = bunchobject.data[:,feature_list]  
    norm_data = normalize_minmax(data)
    data_train, data_test, target_train, target_test = train_test_split( norm_data, bunchobject.target , test_size = size, random_state = seed )
    
    data_train2, data_test2, target_train2, target_test2 = train_test_split(data_test , target_test , test_size = 0.5, random_state = seed)

    model = {}
    accuracy = []
    result = {}
    for k in range(1,21): 
        
        clf = neighbors.KNeighborsClassifier(k)
        clf.fit(data_train, target_train)
        model[k] = clf 
        
        y_pred = clf.predict(data_train2)
        
        matric = get_metrics(target_train2, y_pred, [1,0])
        result[k] = matric
        accuracy.append(matric['accuracy'])
    
    
    k_best = np.argmax(accuracy) + 1

    target_pred = model[k_best].predict(data_test2)
    result_test = get_metrics(target_test2, target_pred, [1,0])
    
    output = {'best k': k_best, 'validation set': result[k_best], 'test set': result_test}


    
    return output
